{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.1\n",
      "4.4.3\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "import bs4 \n",
    "\n",
    "print(bs4.__version__)\n",
    "print(selenium.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "\n",
    "path_to_driver = Path(os.getenv('PYPJ')) / 'Crowler/driver/chromedriver_win32/chromedriver.exe'\n",
    "\n",
    "from linkedin.util.crowler import Crowler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crowler Class Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "import time\n",
    "from xml.dom.minidom import Element \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "\n",
    "path_to_linkedin = 'https://linkedin.com'\n",
    "username = 'frusciante2580@gmail.com'\n",
    "password = 'Utatistics0511'\n",
    "search_word = 'data scientist'\n",
    "path_to_output = os.getcwd() + '\\\\..\\\\..\\\\output\\\\'\n",
    "\n",
    "class Crowler:\n",
    "    def __init__(self, path_to_driver: str, target_url: str, path_to_output: str) -> None:\n",
    "        self.path_to_driver = Service(path_to_driver)\n",
    "        self.target_url = target_url\n",
    "        self.path_to_output = path_to_output\n",
    "\n",
    "        self.driver = webdriver.Chrome(service=self.path_to_driver)\n",
    "        self.wait = WebDriverWait(self.driver, 30)\n",
    "\n",
    "    def login(self, username: str, password: str) -> None:\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "\n",
    "        self.driver.get(self.target_url)\n",
    "        self.driver.maximize_window()\n",
    "\n",
    "        self.driver.find_element(By.CLASS_NAME,'nav__button-secondary').click()\n",
    "        self.driver.find_element(By.ID, 'username').send_keys(self.username)\n",
    "        self.driver.find_element(By.ID, 'password').send_keys(self.password)\n",
    "        self.driver.find_element(By.CLASS_NAME, 'login__form_action_container ').click()\n",
    "    \n",
    "    def search_jobs(self, search_word: str) -> None:\n",
    "        self.search_word = search_word\n",
    "        print(f'searching the jobs: {self.search_word}')\n",
    "  \n",
    "        self.wait.until(expected_conditions.element_to_be_clickable((By.CLASS_NAME,'search-global-typeahead__input')))\n",
    "        self.driver.find_element(By.CLASS_NAME, 'search-global-typeahead__input').send_keys(self.search_word)\n",
    "        self.driver.find_element(By.CLASS_NAME, 'search-global-typeahead__input').send_keys(Keys.ENTER)\n",
    "\n",
    "        while True:\n",
    "            cnt = 1\n",
    "            self.wait.until(expected_conditions.element_to_be_clickable((By.XPATH,f'//*[@id=\"main\"]/div/div/div[{cnt}]/div[2]/a')))\n",
    "            element = self.driver.find_element(By.XPATH, f'//*[@id=\"main\"]/div/div/div[{cnt}]/div[2]/a')\n",
    "            if 'job' in element.text:\n",
    "                element.click()\n",
    "                break \n",
    "            else:\n",
    "                cnt += 1\n",
    "                if cnt > 5:\n",
    "                    print('Jobs not found.')\n",
    "                    break\n",
    "    \n",
    "    def get_urls(self) -> None:\n",
    "        self._implicitly_wait(3)\n",
    "        \n",
    "        page_cnt = 1\n",
    "        while True:\n",
    "            self.urls = []\n",
    "            self.driver.find_element(By.CLASS_NAME, 'scaffold-layout__list-container')\n",
    "            elements = self.driver.find_elements(By.CLASS_NAME, 'jobs-search-results__list-item')\n",
    "            print(f'{len(elements)} elements found in page {page_cnt}.')\n",
    "\n",
    "            for element in elements:\n",
    "                try:\n",
    "                    self._url_getter(element)\n",
    "                \n",
    "                except NoSuchElementException:\n",
    "                    self.driver.execute_script('arguments[0].scrollIntoView({behavior: \"smooth\", block: \"center\"});', element)\n",
    "                    self._implicitly_wait(1)\n",
    "                    self._url_getter(element)\n",
    "            \n",
    "            page_cnt += 1\n",
    "            pages = self.driver.find_element(By.CLASS_NAME, 'artdeco-pagination__pages').find_elements(By.TAG_NAME, 'li')\n",
    "            eop_status = self._page_turnner(page_cnt=page_cnt, elements=pages)\n",
    "            if eop_status:\n",
    "                break \n",
    "        \n",
    "        print(f'collected {len(self.urls)} jobs.')\n",
    "        \n",
    "        return self.urls\n",
    "\n",
    "    def browse_job(self) -> None:\n",
    "        self.job_html_dict = {}\n",
    "        dirname = self.path_to_output + self.search_word.replace(' ', '_')\n",
    "        os.system(f\"mkdir {dirname}\")\n",
    "\n",
    "        for url in self.urls:\n",
    "            self.driver.get(url)\n",
    "            self._job_info_getter()\n",
    "            title = self._job_text_getter()\n",
    "            # self._job_innerHTML_getter()\n",
    "            self._save_to_txt(title)\n",
    "\n",
    "    def _page_turnner(self, page_cnt: int, elements: WebElement) -> bool:\n",
    "        iter_cnt = 0\n",
    "        for page in elements:\n",
    "            page_num = page.find_element(By.TAG_NAME, 'button').text        \n",
    "            if page_num.isdigit(): page_num = int(page_num)\n",
    "    \n",
    "            if page_num == page_cnt:\n",
    "                page.click()\n",
    "                eop_status = False\n",
    "                break\n",
    "\n",
    "            elif page_num == 'â€¦' and iter_cnt > 1:\n",
    "                page.click()\n",
    "                eop_status = False\n",
    "                break\n",
    "               \n",
    "            else:\n",
    "                eop_status = True\n",
    "            \n",
    "            iter_cnt += 1\n",
    "        \n",
    "        print('page number: %s' % str(page_num))\n",
    "        return eop_status  \n",
    "    \n",
    "    def _url_getter(self, element: WebElement) -> None:\n",
    "        url = element.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "        self.urls.append(url)\n",
    "        \n",
    "    def _job_info_getter(self) -> None:\n",
    "        self._implicitly_wait(1)\n",
    "        classname = {\n",
    "            'title': 'jobs-unified-top-card__job-title',\n",
    "            'company_name': 'jobs-unified-top-card__company-name',\n",
    "            'location': 'jobs-unified-top-card__bullet',\n",
    "            'posted_date': 'jobs-unified-top-card__posted-date',\n",
    "            'applicants' : 'jobs-unified-top-card__applicant-count'}\n",
    "              \n",
    "        self.data_job_info = {}\n",
    "        for key in classname.keys():\n",
    "            try:\n",
    "                identifier = classname[key]\n",
    "                src_get = f\"element_{key} = self.driver.find_element(By.CLASS_NAME, '{identifier}')\" \n",
    "                src_append = f\"self.data_job_info['{key}'] = element_{key}.text\" \n",
    "                exec(src_get)\n",
    "                exec(src_append)\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                print(f'{key} not found.')\n",
    "                src_append = f\"'self.data_job_info['{key}'] = None\"\n",
    "                exec(src_append)\n",
    "\n",
    "    def _job_icon_getter(self) -> None:\n",
    "        self._implicitly_wait(1)\n",
    "        \n",
    "    def _job_innerHTML_getter(self) -> str:\n",
    "        self._implicitly_wait(1)\n",
    "        html_data = self.driver.find_element(By.CLASS_NAME,'jobs-box__html-content').get_attribute('innerHTML')\n",
    "        title = self.data_job_info['title'] + '_' +  self.data_job_info['company_name']\n",
    "        self.job_html_dict[title] = html_data\n",
    "        return title\n",
    "\n",
    "    def _job_text_getter(self) -> str:\n",
    "        self._implicitly_wait(1)\n",
    "        text_data = self.driver.find_element(By.CLASS_NAME,'jobs-box__html-content').text\n",
    "        title = self.data_job_info['title'] + '_' +  self.data_job_info['company_name']\n",
    "        self.job_html_dict[title] = text_data\n",
    "        return title\n",
    "    \n",
    "    def _implicitly_wait(self, secs: int) -> None:\n",
    "        time.sleep(secs)\n",
    "\n",
    "    def _save_to_txt(self, title) -> None:\n",
    "        dirname = self.path_to_output + self.search_word.replace(' ', '_')\n",
    "        titlename = re.sub(r'[\\\\|/|:|?|.|\"|<|>|\\|]', '-', title) \n",
    "        filename =  dirname + '\\\\' + titlename.replace(' ','') + '.txt'\n",
    "        with open(file=filename, mode='w', encoding='utf-8') as f:\n",
    "            f.writelines(self.job_html_dict[title])\n",
    "\n",
    "    def run(self, username: str, password: str, search_word: str) -> None:\n",
    "        print('login...')\n",
    "        self.login(username=username, password=password)\n",
    "        self.search_jobs(search_word=search_word)\n",
    "        \n",
    "        print('collecting the URLs...')\n",
    "        result = self.get_urls()\n",
    "        \n",
    "        print(f'browsing the job: place_holder')\n",
    "        self.browse_job()\n",
    "        \n",
    "    def exit(self) -> None:\n",
    "        time.sleep(3)\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "clowler = Crowler(path_to_driver=path_to_driver, target_url=path_to_linkedin, path_to_output=path_to_output)\n",
    "results = clowler.run(username=username, password=password, search_word=search_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DbMaker implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from re import Pattern\n",
    "\n",
    "path_to_output = os.getcwd() + '\\\\..\\\\..\\\\output\\\\'\n",
    "search_word = 'data scientist'\n",
    "\n",
    "class DbMaker:\n",
    "    def __init__(self, path_to_output: str, search_word: str) -> None:\n",
    "        self.path_to_output = path_to_output + search_word.replace(' ', '_') + '\\\\'\n",
    "        self.path_to_txt = path_to_output + search_word.replace(' ', '_') + '\\\\txt\\\\'\n",
    "        self.path_to_json = path_to_output + search_word.replace(' ', '_') + '\\\\json\\\\'\n",
    "        self.dbname = path_to_output + search_word.replace(' ', '_') + '\\\\database.csv'\n",
    "        self.num_of_entries = len(os.listdir(self.path_to_txt))\n",
    "        self.encoding = 'utf-8'\n",
    "        self.newline = \"\"\n",
    "    \n",
    "    def create_csv(self, colnames: list) -> None:\n",
    "        self.default_keys = ['filename', 'title','company_name', 'location','posted_date', 'applicants', 'salary']\n",
    "        self.colnames = colnames\n",
    "        self.header = [*self.default_keys, *self.colnames]\n",
    "        self.header.append('Salary')\n",
    "\n",
    "        with open(file=self.dbname, mode='w', encoding=self.encoding, newline=self.newline) as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "    \n",
    "    def write_to_csv(self) -> None:\n",
    "        with open(file=self.dbname, mode='a', encoding=self.encoding, newline=self.newline) as f:\n",
    "            writer = csv.writer(f)\n",
    "\n",
    "            for jsonfile in os.listdir(self.path_to_json):\n",
    "                txtfile = jsonfile.replace('json','txt')\n",
    "                row = self._write_job_info(jsonfile)\n",
    "                row = self._write_job_contents(txtfile, row)\n",
    "                writer.writerow(row)\n",
    "\n",
    "    def _write_job_info(self, jsonfile: str) -> list:\n",
    "        row = [jsonfile.replace('.json','')]\n",
    "        with open(file=self.path_to_json + jsonfile, mode='r',encoding=self.encoding) as fp:\n",
    "            job_info_dict = json.load(fp)\n",
    "            for key in job_info_dict.keys():\n",
    "                row.append(job_info_dict[key])\n",
    "            return row\n",
    "            \n",
    "    def _write_job_contents(self, txtfile: str, row: list) -> list:\n",
    "        with open(file=self.path_to_txt + txtfile, mode='r', encoding=self.encoding) as f:\n",
    "            job_info_txt = f.readlines()\n",
    "            compiled = re.compile('\\D*\\d\\d,000\\D*')\n",
    "\n",
    "            for word in self.colnames:\n",
    "                status = False\n",
    "                row = self._get_bool_info(row, job_info_txt, status, word)\n",
    "            \n",
    "            row = self._get_saraly(row, job_info_txt, compiled)\n",
    "\n",
    "        return row\n",
    "    \n",
    "    def _get_bool_info(self, row: list, job_info_txt: list, status: bool, word: str) -> list:\n",
    "        for line in job_info_txt:\n",
    "            if word in line:\n",
    "                status = True\n",
    "                break\n",
    "        row.append(status)\n",
    "        return row \n",
    "\n",
    "    def _get_saraly(self, row: list, job_info_txt: list, compiled: Pattern) -> list:\n",
    "        for line in job_info_txt:\n",
    "            matched  = compiled.match(line)\n",
    "            try:\n",
    "                salary = re.sub(r'\\D', '', matched.group())\n",
    "                row.append(salary)\n",
    "                break\n",
    "            except AttributeError:\n",
    "                pass\n",
    "        \n",
    "        row.append(None)\n",
    "        return row\n",
    "\n",
    "    def run(self, colnames: list):\n",
    "        self.create_csv(colnames)\n",
    "        self.write_to_csv()\n",
    " \n",
    "\n",
    "colnames = ['Python', 'SQL', 'SAS', 'C++','MSc', 'PhD', 'Cloud', 'AWS', 'Azure', 'Statistics','Computer Science', 'CS', 'consulting'] \n",
    "dbmaker = DbMaker(path_to_output=path_to_output, search_word=search_word)\n",
    "dbmaker.run(colnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e81a980c7a1d445ec0e0046744d3c03fc970dcf62d341826350f41afd9adf026"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
